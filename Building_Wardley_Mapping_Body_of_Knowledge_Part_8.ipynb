{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tractorjuice/Building_BoK/blob/main/Building_Wardley_Mapping_Body_of_Knowledge_Part_8_GitHub_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07c1e3b9"
      },
      "source": [
        "# Wardley Mapping Body of Knowledge Using Langchain & OpenAI\n",
        "## Part 8, adding open Wardley Maps to the body of knowledge\n",
        "This example shows how to create and query an internal knowledge base using ChatGPT.\n",
        "\n",
        "This does not require a GPU runtime."
      ],
      "id": "07c1e3b9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UqlAAxTXnGF"
      },
      "source": [
        "## Set Up\n"
      ],
      "id": "0UqlAAxTXnGF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive for data storage"
      ],
      "metadata": {
        "id": "99mJDRkd3bKe"
      },
      "id": "99mJDRkd3bKe"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "lsHuXTpJ3WZI"
      },
      "id": "lsHuXTpJ3WZI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup file structure"
      ],
      "metadata": {
        "id": "PqChQ5xy3WGi"
      },
      "id": "PqChQ5xy3WGi"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "DOCS_FOLDER = \"/content/gdrive/MyDrive/WardleyKB\"  # Google drive folder to save the audio clips from YouTube videos\n",
        "MAPS_FOLDER = os.path.join(DOCS_FOLDER, \"maps/research2022\")  # Sub-directory for audio files\n",
        "DATASTORE = os.path.join(DOCS_FOLDER, \"maps/datastore\")  # Sub-directory for audio files\n",
        "AUDIO_FOLDER = os.path.join(DOCS_FOLDER, \"audio\")  # Sub-directory for audio files\n",
        "TRANSCRIPTS_FOLDER = os.path.join(AUDIO_FOLDER, \"transcripts\")  # Sub-directory for audio files\n",
        "TRANSCRIPTS_TEXT_FOLDER = os.path.join(TRANSCRIPTS_FOLDER, \"text\")  # Sub-directory for audio files\n",
        "TRANSCRIPTS_WHISPER_FOLDER = os.path.join(TRANSCRIPTS_FOLDER, \"whisper_chunks\")  # Sub-directory for audio files\n",
        "\n",
        "# Check if directory exists and if not, create it\n",
        "if not os.path.exists(DOCS_FOLDER):\n",
        "    os.makedirs(DOCS_FOLDER)\n",
        "\n",
        "# Check if directory exists and if not, create it\n",
        "if not os.path.exists(MAPS_FOLDER):\n",
        "    os.makedirs(MAPS_FOLDER)\n",
        "\n",
        "# Check if directory exists and if not, create it\n",
        "if not os.path.exists(DATASTORE):\n",
        "    os.makedirs(DATASTORE)\n",
        "\n",
        "# Check if sub-directory for audio exists and if not, create it\n",
        "if not os.path.exists(AUDIO_FOLDER):\n",
        "    os.makedirs(AUDIO_FOLDER)\n",
        "\n",
        "# Check if sub-directory for audio exists and if not, create it\n",
        "if not os.path.exists(TRANSCRIPTS_FOLDER):\n",
        "    os.makedirs(TRANSCRIPTS_FOLDER)\n",
        "\n",
        "# Check if sub-directory for audio exists and if not, create it\n",
        "if not os.path.exists(TRANSCRIPTS_TEXT_FOLDER):\n",
        "    os.makedirs(TRANSCRIPTS_TEXT_FOLDER)\n",
        "\n",
        "# Check if sub-directory for audio exists and if not, create it\n",
        "if not os.path.exists(TRANSCRIPTS_WHISPER_FOLDER):\n",
        "    os.makedirs(TRANSCRIPTS_WHISPER_FOLDER)"
      ],
      "metadata": {
        "id": "OOgRU-kP3TT9"
      },
      "id": "OOgRU-kP3TT9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required dependencies"
      ],
      "metadata": {
        "id": "lN2KnixvgwyX"
      },
      "id": "lN2KnixvgwyX"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain\n",
        "!pip install -q openai\n",
        "!pip install -q tiktoken"
      ],
      "metadata": {
        "id": "EwkyLN8kgr1l"
      },
      "id": "EwkyLN8kgr1l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup required API keys"
      ],
      "metadata": {
        "id": "Lx8S7nHf9LB8"
      },
      "id": "Lx8S7nHf9LB8"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\" # Put your OpenAI API key here\n",
        "\n",
        "#MODEL = \"gpt-3\"\n",
        "#MODEL = \"gpt-3.5-turbo\"\n",
        "#MODEL = \"gpt-3.5-turbo-0613\"\n",
        "#MODEL = \"gpt-3.5-turbo-16k\"\n",
        "MODEL = \"gpt-3.5-turbo-16k-0613\"\n",
        "#MODEL = \"gpt-4\"\n",
        "#MODEL = \"gpt-4-0613\"\n",
        "#MODEL = \"gpt-4-32k-0613\""
      ],
      "metadata": {
        "id": "GxXUFlwS9IW3"
      },
      "id": "GxXUFlwS9IW3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required dependencies for GitHub"
      ],
      "metadata": {
        "id": "HnctPjvD94i6"
      },
      "id": "HnctPjvD94i6"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install PyGithub"
      ],
      "metadata": {
        "id": "MvfGSnDFzoi5"
      },
      "id": "MvfGSnDFzoi5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from github import Github\n",
        "import base64"
      ],
      "metadata": {
        "id": "i-PDE5ZHX9bb"
      },
      "id": "i-PDE5ZHX9bb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the required GitHub repo and setup API keys"
      ],
      "metadata": {
        "id": "fENTpJikxZri"
      },
      "id": "fENTpJikxZri"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tY7CJZoh5cma"
      },
      "outputs": [],
      "source": [
        "GITHUBREPO = \"swardley/Research2022\" # Source of Wardley Maps\n",
        "GITHUB = \"\" # Put your GitHub API key here"
      ],
      "id": "tY7CJZoh5cma"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35f99145"
      },
      "source": [
        "## Wardley Map Data Collection"
      ],
      "id": "35f99145"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate GitHub repository\n",
        "\n",
        "g = Github(GITHUB)\n",
        "repo = g.get_repo(GITHUBREPO)"
      ],
      "metadata": {
        "id": "DD6laupRyToO"
      },
      "id": "DD6laupRyToO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a list of all the available files in GitHub\n",
        "\n",
        "file_list = []\n",
        "contents = repo.get_contents(\"\")\n",
        "while contents:\n",
        "    file_item = contents.pop(0)\n",
        "    if file_item.type == \"dir\":\n",
        "        contents.extend(repo.get_contents(file_item.path))\n",
        "    else:\n",
        "        file_name = file_item.name\n",
        "        # Ignore files that are not maps\n",
        "        if not file_name.isupper() and not file_name.startswith('.') and file_name.lower() != 'readme.md':\n",
        "            file_list.append(file_item.path)\n",
        "\n",
        "print (file_list)"
      ],
      "metadata": {
        "id": "62Bs9iT8zClk"
      },
      "id": "62Bs9iT8zClk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a the files and save them to the Google Drive\n",
        "\n",
        "for file in file_list:\n",
        "    file_item = repo.get_contents(file)\n",
        "    file_content = base64.b64decode(file_item.content).decode('utf-8')\n",
        "    maps_filename = f'{MAPS_FOLDER}/{file}.owm'\n",
        "\n",
        "    os.makedirs(os.path.dirname(maps_filename), exist_ok=True)\n",
        "\n",
        "    if not os.path.isfile(maps_filename):\n",
        "        with open(maps_filename, 'w') as f:\n",
        "            f.write(file_content)\n",
        "            print (file)\n",
        "    else:\n",
        "        print (f\"File already exists: {maps_filename}\")"
      ],
      "metadata": {
        "id": "-vKpGpww1sbT"
      },
      "id": "-vKpGpww1sbT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split text and upsert the maps into Pinecone or FAISS vector database"
      ],
      "metadata": {
        "id": "9WGCG4Jg8sTn"
      },
      "id": "9WGCG4Jg8sTn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03__ie72B8J_"
      },
      "source": [
        "Initialise preferred vector database"
      ],
      "id": "03__ie72B8J_"
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = 'FAISS' # Set to 'Pinecone' or 'FAISS' for the vector datbase. If using FAISS, no GPU required"
      ],
      "metadata": {
        "id": "N6nUw7Cmf6Fh"
      },
      "id": "N6nUw7Cmf6Fh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if vectorstore == 'Pinecone':\n",
        "    !pip install -q pinecone-client\n",
        "    from langchain.vectorstores import Pinecone\n",
        "    from tqdm.auto import tqdm\n",
        "    import pinecone\n",
        "\n",
        "    # initialize pinecone\n",
        "    pinecone.init(\n",
        "        api_key=\"\",  # find at app.pinecone.io\n",
        "        environment=\"us-west4-gcp-free\"  # next to api key in console\n",
        "        )\n",
        "\n",
        "    index_name = \"knowledge\" # Put your Pincecone index name here\n",
        "    name_space = \"researchmaps\" # Put your Pincecone namespace here\n",
        "\n",
        "else:\n",
        "    !pip install -q faiss-cpu\n",
        "    from langchain.vectorstores import FAISS\n"
      ],
      "metadata": {
        "id": "46ZWnrX5f7Di"
      },
      "id": "46ZWnrX5f7Di",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install all required dependencies"
      ],
      "metadata": {
        "id": "gr8YiIxH9jav"
      },
      "id": "gr8YiIxH9jav"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6TkUqCJB_2X"
      },
      "outputs": [],
      "source": [
        "#import json\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "import tiktoken"
      ],
      "id": "W6TkUqCJB_2X"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walk through files and upsert into preferred vector database"
      ],
      "metadata": {
        "id": "Dp_1I1DQ9oBr"
      },
      "id": "Dp_1I1DQ9oBr"
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 0\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20, separator=\"\\n\")\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "total_maps = len(file_list)\n",
        "\n",
        "for files in file_list:\n",
        "    counter = counter + 1\n",
        "    maps_filename = f'{MAPS_FOLDER}/{files}.owm'\n",
        "\n",
        "    # Open the file and read its content\n",
        "    with open(maps_filename, 'r') as f:\n",
        "        file_content = f.read()\n",
        "\n",
        "    print(f'\\n{counter} of {total_maps}: Loading {maps_filename} ......')\n",
        "    docs = []\n",
        "    metadatas = []\n",
        "\n",
        "    splits = text_splitter.split_text(file_content)\n",
        "    docs.extend(splits)\n",
        "\n",
        "    # Extend metadatas with a metadata dict for each split\n",
        "    metadatas.extend([{\"source\": maps_filename}] * len(splits))\n",
        "\n",
        "    # Upsert one at a time, this handles errors with OpenAI API better\n",
        "\n",
        "    if vectorstore == 'Pinecone':\n",
        "        try:\n",
        "            vector_store = Pinecone.from_texts(docs, embeddings, metadatas=metadatas, index_name=index_name, namespace=name_space)\n",
        "        except:\n",
        "            print(\"Error upserting data into the vectorstore\\n\")\n",
        "    else:\n",
        "        try:\n",
        "            vector_store = FAISS.from_texts(docs, embeddings, metadatas=metadatas)\n",
        "            if os.path.exists(f\"{DATASTORE}/index.faiss\"):\n",
        "                existing_index=FAISS.load_local(f\"{DATASTORE}\", embeddings)\n",
        "                existing_index.merge_from(vector_store)\n",
        "                existing_index.save_local(f\"{DATASTORE}\")\n",
        "            else:\n",
        "                vector_store.save_local(f\"{DATASTORE}\") # Download the files `$DATA_STORE_DIR/index.faiss` and `$DATA_STORE_DIR/index.pkl` to local\n",
        "        except:\n",
        "            print(\"Error upserting data into the vectorstore\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "XeD4XGto9srp"
      },
      "id": "XeD4XGto9srp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdD6zcvNFX9B"
      },
      "source": [
        "### Query using the vector store with ChatGPT integration"
      ],
      "id": "OdD6zcvNFX9B"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup access to the preferred vector database"
      ],
      "metadata": {
        "id": "S5Vf831zQLJa"
      },
      "id": "S5Vf831zQLJa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibz6Tz_7iPeQ"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "id": "ibz6Tz_7iPeQ"
    },
    {
      "cell_type": "code",
      "source": [
        "if vectorstore == 'Pinecone':\n",
        "    vector_store = Pinecone.from_existing_index(index_name, embeddings, namespace=name_space)\n",
        "\n",
        "else:\n",
        "    # Open FAISS datastore\n",
        "    from langchain.vectorstores import FAISS\n",
        "    if os.path.exists(f\"{DATASTORE}\"):\n",
        "        vector_store = FAISS.load_local(\n",
        "            f\"{DATASTORE}\",\n",
        "            OpenAIEmbeddings()\n",
        "            )\n",
        "    else:\n",
        "        print(f\"Missing files. Upload index.faiss and index.pkl files to data_store directory first\")"
      ],
      "metadata": {
        "id": "4YOelIBZiw0m"
      },
      "id": "4YOelIBZiw0m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNjGnZME_DNU"
      },
      "source": [
        "## Example Queries (Q&A)"
      ],
      "id": "eNjGnZME_DNU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup the prompt"
      ],
      "metadata": {
        "id": "WDcuSm-wQPC-"
      },
      "id": "WDcuSm-wQPC-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32a49412"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "system_template=\"\"\"\n",
        "    You are SimonGPT a strategy researcher based in the UK.\n",
        "    “Researcher” means in the style of a strategy researcher with well over twenty years research in strategy, wardley mapping and cloud computing.\n",
        "    You use examples from Wardley Mapping in your answers.\n",
        "    Your language should be for an 12 year old to understand.\n",
        "    If you do not know the answer to a question, do not make information up - instead, ask a follow-up question in order to gain more context.\n",
        "    Use a mix of technical and colloquial uk english language to create an accessible and engaging tone.\n",
        "    Use the following pieces of context to answer the users question.\n",
        "    ----------\n",
        "    {summaries}\n",
        "    \"\"\"\n",
        "messages = [\n",
        "    SystemMessagePromptTemplate.from_template(system_template),\n",
        "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "]\n",
        "prompt = ChatPromptTemplate.from_messages(messages)"
      ],
      "id": "32a49412"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialise the LLM API"
      ],
      "metadata": {
        "id": "jlIfoZccQRmm"
      },
      "id": "jlIfoZccQRmm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3018f865"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "\n",
        "chain_type_kwargs = {\"prompt\": prompt}\n",
        "llm = ChatOpenAI(model_name=MODEL, temperature=0, max_tokens=1000)  # Modify model_name if you have access to GPT-4\n",
        "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vector_store.as_retriever(),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs=chain_type_kwargs\n",
        ")"
      ],
      "id": "3018f865"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example question"
      ],
      "metadata": {
        "id": "Fb5eUUo4zKet"
      },
      "id": "Fb5eUUo4zKet"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "032a47f8"
      },
      "outputs": [],
      "source": [
        "query = \"how is AI used in these maps?\"\n",
        "result = chain(query)"
      ],
      "id": "032a47f8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the answer and sources"
      ],
      "metadata": {
        "id": "6_aZT0JXPr_9"
      },
      "id": "6_aZT0JXPr_9"
    },
    {
      "cell_type": "code",
      "source": [
        "print('Question:', result['question'])\n",
        "print('Answer:  ', result['answer'],'\\n')\n",
        "\n",
        "source_documents = result['source_documents']\n",
        "for index, document in enumerate(source_documents):\n",
        "    if 'source' in document.metadata:\n",
        "        print(f\"Source {index + 1}:\", document.metadata['source'])\n",
        "        #print(f\"Content: {document.page_content}\")"
      ],
      "metadata": {
        "id": "z4_BxSv6jrku"
      },
      "id": "z4_BxSv6jrku",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Are there any common components across these maps?\"\n",
        "result = chain(query)"
      ],
      "metadata": {
        "id": "e5IEjha3xgn0"
      },
      "id": "e5IEjha3xgn0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Question:', result['question'])\n",
        "print('Answer:  ', result['answer'],'\\n')\n",
        "\n",
        "source_documents = result['source_documents']\n",
        "for index, document in enumerate(source_documents):\n",
        "    if 'source' in document.metadata:\n",
        "        print(f\"Source {index + 1}:\", document.metadata['source'])\n",
        "        #print(f\"Content: {document.page_content}\")"
      ],
      "metadata": {
        "id": "WTFIPYQpxiRn"
      },
      "id": "WTFIPYQpxiRn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the opportunities for SMEs within these maps?\"\n",
        "result = chain(query)"
      ],
      "metadata": {
        "id": "0GaMqTO4yaqy"
      },
      "execution_count": null,
      "outputs": [],
      "id": "0GaMqTO4yaqy"
    },
    {
      "cell_type": "code",
      "source": [
        "print('Question:', result['question'])\n",
        "print('Answer:  ', result['answer'],'\\n')\n",
        "\n",
        "source_documents = result['source_documents']\n",
        "for index, document in enumerate(source_documents):\n",
        "    if 'source' in document.metadata:\n",
        "        print(f\"Source {index + 1}:\", document.metadata['source'])\n",
        "        #print(f\"Content: {document.page_content}\")"
      ],
      "metadata": {
        "id": "z95pCLs-yeOm"
      },
      "id": "z95pCLs-yeOm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"I want to build a component that can be used by all of these maps, what should I create?\"\n",
        "result = chain(query)"
      ],
      "metadata": {
        "id": "JS7y7Ol0zbrF"
      },
      "id": "JS7y7Ol0zbrF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Question:', result['question'])\n",
        "print('Answer:  ', result['answer'],'\\n')\n",
        "\n",
        "source_documents = result['source_documents']\n",
        "for index, document in enumerate(source_documents):\n",
        "    if 'source' in document.metadata:\n",
        "        print(f\"Source {index + 1}:\", document.metadata['source'])\n",
        "        #print(f\"Content: {document.page_content}\")"
      ],
      "metadata": {
        "id": "brnC79KfzbeQ"
      },
      "id": "brnC79KfzbeQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete the Pinecone namespace if required\n",
        "\n",
        "#vector_store.delete(delete_all=True, namespace=name_space)"
      ],
      "metadata": {
        "id": "43p0rgjmMjY3"
      },
      "id": "43p0rgjmMjY3",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuClass": "premium",
      "collapsed_sections": [
        "eNjGnZME_DNU"
      ],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "b1677b440931f40d89ef8be7bf03acb108ce003de0ac9b18e8d43753ea2e7103"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
